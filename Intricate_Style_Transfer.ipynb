{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intricate-Style-Transfer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dInkcx9XfMgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tensorflow==1.8.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5sRpGy_xsIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install tensorflow-gpu==1.8.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtNtVXuYx5F-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "# !dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "# !apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
        "# !apt-get update\n",
        "# !apt-get install cuda=9.0.176-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aYOOY3YpGt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !nvcc --version"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6vaAsxukJYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install scipy==1.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u2O_YxXASnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlCXTV5CGlsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "from __future__ import absolute_import\n",
        "\n",
        "# from imageio import imread, imwrite\n",
        "# from skimage.transform import resize\n",
        "from scipy.misc import imread, imresize, imsave\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "import numpy as np \n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "from PIL import Image\n",
        "import PIL.ImageOps\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.keras._impl.keras.models import Model\n",
        "from tensorflow.python.keras._impl.keras.engine import Input\n",
        "from tensorflow.python.keras._impl.keras.layers.convolutional import Convolution2D, AveragePooling2D, MaxPooling2D\n",
        "from tensorflow.python.keras._impl.keras import backend as K\n",
        "from tensorflow.python.keras._impl.keras.utils.data_utils import get_file\n",
        "from tensorflow.python.keras._impl.keras.utils.layer_utils import convert_all_kernels_in_model\n",
        "from tensorflow.python.keras._impl.keras.applications.vgg16 import VGG16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "060iJyCcpEFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VQHl2-aZISB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "TF_19_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jXBG0tNnUe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hex_to_rgb(h):\n",
        "      hh = h.lstrip(\"#\")\n",
        "      rgb = tuple(int(hh[i:i+2], 16) for i in (0, 2 ,4))\n",
        "      return rgb\n",
        "\n",
        "def str_to_bool(v):\n",
        "    return v.lower() in (\"true\", \"yes\", \"t\", \"1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYMlrDSoVYA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "file_list = os.listdir('/content/drive/My Drive/intricate-art-neural-transfer/EssenceOutputs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzouFELJ5O3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "descriptions = np.load(\"/content/drive/My Drive/intricate-art-neural-transfer/descriptions.npy\")\n",
        "top3 = np.load(\"/content/drive/My Drive/intricate-art-neural-transfer/top3.npy\")\n",
        "painting_emotions = pd.read_pickle(\"/content/drive/My Drive/intricate-art-neural-transfer/painting_emotions.pkl\")\n",
        "word_emotions = pd.read_pickle(\"/content/drive/My Drive/intricate-art-neural-transfer/word_emotions.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9uH8spr8cg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "final_file_list = []\n",
        "file_styles = []\n",
        "for i in file_list:\n",
        "  index = int(i.split(\".\")[0].split(\"-\")[-1])\n",
        "  person_emotions = [np.array(word_emotions.loc[j].tolist()) for j in top3[index] if j in word_emotions.index.tolist()]\n",
        "  if not person_emotions:\n",
        "    continue\n",
        "\n",
        "  person_emotion = np.mean(person_emotions, axis = 0)\n",
        "  \n",
        "  final_file_list.append(i)\n",
        "  nn = NearestNeighbors(1).fit(painting_emotions)\n",
        "  dists, idxs = nn.kneighbors(person_emotion.reshape(1,10))\n",
        "  file_styles.append(painting_emotions.iloc[idxs[0][0]].name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfPRMp0g_TEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ssl\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "for i in range(len(file_styles)):\n",
        "  index = int(final_file_list[i].split(\".\")[0].split(\"-\")[-1])\n",
        "  urllib.request.urlretrieve(file_styles[i], \"/content/drive/My Drive/intricate-art-neural-transfer/EssenceStyles/essence-\" + str(index) + \"-style.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL_-cqJlB3Ag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "963dddb6-1751-477f-e999-2fec5e6c5be7"
      },
      "source": [
        "print(final_file_list[i])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "essence-137.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGVt8ZYMpcdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i=34\n",
        "home_dir = '/content/drive/My Drive/intricate-art-neural-transfer/'\n",
        "image_name = final_file_list[i]\n",
        "index = int(final_file_list[i].split(\".\")[0].split(\"-\")[-1])\n",
        "base_image_path = home_dir + 'EssenceOutputs/' + image_name\n",
        "# base_image_path = home_dir + 'silhouettes/ballerina.jpg'\n",
        "style_reference_image_paths = [\"/content/drive/My Drive/intricate-art-neural-transfer/style/essence-\" + str(index) + \"-style.png\"]\n",
        "result_prefix = home_dir + 'EssenceStyles/styled-' + image_name.split(\".\")[0]\n",
        "num_iter = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aD9NpBzfzra",
        "colab_type": "code",
        "outputId": "8bb00ac1-478c-4dce-fef9-c14a31ddddd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#  def intricate_style_transfer(base_image_path, style_reference_image_paths, result_prefix, num_iter): \n",
        "content_weight = 0.025\n",
        "total_variation_weight = 8.5e-5\n",
        "style_weight = [1]\n",
        "style_scale = 1.0\n",
        "content_loss_type = 0\n",
        "img_size = 600\n",
        "model = \"vgg16\"\n",
        "content_layer = \"block5_conv2\"\n",
        "init_image = \"content\"\n",
        "pool = \"max\"\n",
        "rescale_method = \"bilinear\"\n",
        "rescale_image = str_to_bool(\"False\")\n",
        "maintain_aspect_ratio = str_to_bool(\"True\")\n",
        "bg_color = hex_to_rgb('#ffffff')\n",
        "min_improvement = 0.0\n",
        "bg_image = None\n",
        "\n",
        "silhouette = Image.open(base_image_path).convert('L')\n",
        "inverted_silhouette = PIL.ImageOps.invert(silhouette)\n",
        "\n",
        "style_image_paths = []\n",
        "for style_image_path in style_reference_image_paths:\n",
        "    style_image_paths.append(style_image_path)\n",
        "\n",
        "style_weights = []\n",
        "\n",
        "if len(style_image_paths) != len(style_weight):\n",
        "    print(\"Mismatch in number of style images provided and number of style weights provided. \\n\"\n",
        "          \"Found %d style images and %d style weights. \\n\"\n",
        "          \"Equally distributing weights to all other styles.\" % (len(style_image_paths), len(style_weight)))\n",
        "\n",
        "    weight_sum = sum(style_weight) * style_scale\n",
        "    count = len(style_image_paths)\n",
        "\n",
        "    for i in range(len(style_image_paths)):\n",
        "        style_weights.append(weight_sum / count)\n",
        "else:\n",
        "    for style_weight in style_weight:\n",
        "        style_weights.append(style_weight * style_scale)\n",
        "\n",
        "# Decide pooling function\n",
        "pooltype = str(pool).lower()\n",
        "assert pooltype in [\"ave\", \"max\"], 'Pooling argument is wrong. Needs to be either \"ave\" or \"max\".'\n",
        "\n",
        "pooltype = 1 if pooltype == \"ave\" else 0\n",
        "\n",
        "read_mode = \"gray\" if init_image == \"gray\" else \"color\"\n",
        "\n",
        "# dimensions of the generated picture.\n",
        "img_width = img_height = 0\n",
        "\n",
        "img_WIDTH = img_HEIGHT = 0\n",
        "aspect_ratio = 0\n",
        "\n",
        "assert content_loss_type in [0, 1, 2], \"Content Loss Type must be one of 0, 1 or 2\"\n",
        "\n",
        "# util function to open, resize and format pictures into appropriate tensors\n",
        "def preprocess_image(image_path, load_dims=False, read_mode=\"color\"):\n",
        "    global img_width, img_height, img_WIDTH, img_HEIGHT, aspect_ratio\n",
        "\n",
        "    mode = \"RGB\" if read_mode == \"color\" else \"L\"\n",
        "    img = imread(image_path, mode=mode)  # Prevents crashes due to PNG images (ARGB)\n",
        "\n",
        "    if mode == \"L\":\n",
        "        # Expand the 1 channel grayscale to 3 channel grayscale image\n",
        "        temp = np.zeros(img.shape + (3,), dtype=np.uint8)\n",
        "        temp[:, :, 0] = img\n",
        "        temp[:, :, 1] = img.copy()\n",
        "        temp[:, :, 2] = img.copy()\n",
        "\n",
        "        img = temp\n",
        "\n",
        "    if load_dims:\n",
        "        img_WIDTH = img.shape[0]\n",
        "        img_HEIGHT = img.shape[1]\n",
        "        aspect_ratio = float(img_HEIGHT) / img_WIDTH\n",
        "\n",
        "        img_width = img_size\n",
        "        if maintain_aspect_ratio:\n",
        "            img_height = int(img_width * aspect_ratio)\n",
        "        else:\n",
        "            img_height = img_size\n",
        "\n",
        "    img = imresize(img, (img_width, img_height)).astype('float32')\n",
        "\n",
        "    # RGB -> BGR\n",
        "    img = img[:, :, ::-1]\n",
        "\n",
        "    img[:, :, 0] -= 103.939\n",
        "    img[:, :, 1] -= 116.779\n",
        "    img[:, :, 2] -= 123.68\n",
        "\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    x = x.reshape((img_width, img_height, 3))\n",
        "\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "\n",
        "    # BGR -> RGB\n",
        "    x = x[:, :, ::-1]\n",
        "\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "\n",
        "def load_mask_sil(invert_sil, shape):\n",
        "    width, height, _ = shape\n",
        "    invert_array = np.array(invert_sil.convert('L'))\n",
        "    mask = imresize(invert_sil, (width, height), interp='bicubic').astype('float32')\n",
        "\n",
        "    # Perform binarization of mask\n",
        "    mask[mask <= 127] = 0\n",
        "    mask[mask > 128] = 255\n",
        "\n",
        "    max = np.amax(mask)\n",
        "    mask /= max\n",
        "\n",
        "    return mask\n",
        "\n",
        "  \n",
        "\n",
        "# util function to apply mask to generated image\n",
        "def mask_content(content_path, generated, mask, bg_color=bg_color):\n",
        "    content_image = imread(content_path, mode='RGB')\n",
        "    content_image = imresize(content_image, (img_width, img_height), interp='bicubic')\n",
        "    width, height, channels = generated.shape\n",
        "    if bg_image is not None:\n",
        "        background_image = imread(bg_image, mode='RGB')\n",
        "        background_image = imresize(background_image, (img_width, img_height), interp='bicubic')\n",
        "        for i in range(width):\n",
        "            for j in range(height):\n",
        "                if mask[i,j] == 0:\n",
        "                    generated[i, j, :] = background_image[i, j, :]\n",
        "    else:\n",
        "        for i in range(width):\n",
        "            for j in range(height):\n",
        "                if mask[i, j] == 0.:\n",
        "                    for k in range(3):\n",
        "                        generated[i,j][k] = bg_color[k]\n",
        "\n",
        "    return generated\n",
        "\n",
        "\n",
        "\n",
        "def pooling_func(x):\n",
        "    if pooltype == 1:\n",
        "        return AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "    else:\n",
        "        return MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "\n",
        "\n",
        "# get tensor representations of our images\n",
        "base_image = K.variable(preprocess_image(base_image_path, True, read_mode=read_mode))\n",
        "print(base_image.shape)\n",
        "\n",
        "style_reference_images = []\n",
        "for style_path in style_image_paths:\n",
        "    style_reference_images.append(K.variable(preprocess_image(style_path)))\n",
        "\n",
        "# this will contain our generated image\n",
        "combination_image = K.placeholder((1, img_width, img_height, 3))\n",
        "\n",
        "image_tensors = [base_image]\n",
        "for style_image_tensor in style_reference_images:\n",
        "    image_tensors.append(style_image_tensor)\n",
        "image_tensors.append(combination_image)\n",
        "\n",
        "nb_tensors = len(image_tensors)\n",
        "print(\"nb_tensors\", nb_tensors)\n",
        "nb_style_images = nb_tensors - 2 # Content and Output image not considered\n",
        "\n",
        "# combine the various images into a single Keras tensor\n",
        "input_tensor = K.concatenate(image_tensors, axis=0)\n",
        "\n",
        "shape = (nb_tensors, img_width, img_height, 3)\n",
        "inp_shape = (img_width, img_height, 3)\n",
        "\n",
        "ip = Input(tensor=input_tensor, batch_shape=shape)\n",
        "\n",
        "model = VGG16(include_top=False, weights='imagenet', input_tensor = input_tensor,\n",
        "              input_shape=inp_shape, pooling='max')\n",
        "\n",
        "print('Model loaded.')\n",
        "\n",
        "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "shape_dict = dict([(layer.name, layer.output_shape) for layer in model.layers])\n",
        "\n",
        "# compute the neural style loss\n",
        "# first we need to define 4 util functions\n",
        "\n",
        "# Improvement 1\n",
        "# the gram matrix of an image tensor (feature-wise outer product) using shifte, return_mask_img = Trued activations\n",
        "def gram_matrix(x):\n",
        "    assert K.ndim(x) == 3\n",
        "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "    gram = K.dot(features - 1, K.transpose(features - 1))\n",
        "    return gram\n",
        "\n",
        "\n",
        "# the \"style loss\" is designed to maintain\n",
        "# the style of the reference image in the generated image.\n",
        "# It is based on the gram matrices (which capture style) of\n",
        "# feature maps from the style reference image\n",
        "# and from the generated image\n",
        "\n",
        "def style_loss(style, combination, nb_channels=None):\n",
        "    assert K.ndim(style) == 3\n",
        "    assert K.ndim(combination) == 3\n",
        "\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(combination)\n",
        "    channels = 3\n",
        "    size = img_width * img_height\n",
        "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
        "\n",
        "\n",
        "# an auxiliary loss function\n",
        "# designed to maintain the \"content\" of the\n",
        "# base image in the generated image\n",
        "def content_loss(base, combination):\n",
        "    channel_dim = -1\n",
        "\n",
        "    try:\n",
        "        channels = K.int_shape(base)[channel_dim]\n",
        "    except TypeError:\n",
        "        channels = K.shape(base)[channel_dim]\n",
        "    size = img_width * img_height\n",
        "\n",
        "    if content_loss_type == 1:\n",
        "        multiplier = 1. / (2. * (channels ** 0.5) * (size ** 0.5))\n",
        "    elif content_loss_type == 2:\n",
        "        multiplier = 1. / (channels * size)\n",
        "    else:\n",
        "        multiplier = 1.\n",
        "\n",
        "    return multiplier * K.sum(K.square(combination - base))\n",
        "\n",
        "\n",
        "# the 3rd loss function, total variation loss,\n",
        "# designed to keep the generated image locally coherent\n",
        "def total_variation_loss(x):\n",
        "    assert K.ndim(x) == 4\n",
        "    a = K.square(x[:, :img_width - 1, :img_height - 1, :] - x[:, 1:, :img_height - 1, :])\n",
        "    b = K.square(x[:, :img_width - 1, :img_height - 1, :] - x[:, :img_width - 1, 1:, :])\n",
        "    return K.sum(K.pow(a + b, 1.25))\n",
        "\n",
        "if model == \"vgg19\":\n",
        "    feature_layers = ['block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2',\n",
        "                  'block3_conv3', 'block3_conv4', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_conv4',\n",
        "                  'block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_conv4']\n",
        "else:\n",
        "\n",
        "    feature_layers = ['block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2',\n",
        "                      'block3_conv3', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block5_conv1', 'block5_conv2',\n",
        "                      'block5_conv3']\n",
        "\n",
        "# combine these loss functions into a single scalar\n",
        "loss = K.variable(0.)\n",
        "layer_features = outputs_dict[content_layer]\n",
        "base_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[nb_tensors - 1, :, :, :]\n",
        "loss = loss + content_weight * content_loss(base_image_features,\n",
        "                                      combination_features)\n",
        "# Improvement 2\n",
        "# Use all layers for style feature extraction and reconstruction\n",
        "nb_layers = len(feature_layers) - 1\n",
        "\n",
        "channel_index =  -1\n",
        "\n",
        "# Improvement 3 : Chained Inference without blurring\n",
        "for i in range(len(feature_layers) - 1):\n",
        "    layer_features = outputs_dict[feature_layers[i]]\n",
        "    shape = shape_dict[feature_layers[i]]\n",
        "    combination_features = layer_features[nb_tensors - 1, :, :, :]\n",
        "    style_reference_features = layer_features[1:nb_tensors - 1, :, :, :]\n",
        "    sl1 = []\n",
        "    for j in range(nb_style_images):\n",
        "        sl1.append(style_loss(style_reference_features[j], combination_features,  shape))\n",
        "\n",
        "    layer_features = outputs_dict[feature_layers[i + 1]]\n",
        "    shape = shape_dict[feature_layers[i + 1]]\n",
        "    combination_features = layer_features[nb_tensors - 1, :, :, :]\n",
        "    style_reference_features = layer_features[1:nb_tensors - 1, :, :, :]\n",
        "    sl2 = []\n",
        "    for j in range(nb_style_images):\n",
        "        sl2.append(style_loss(style_reference_features[j], combination_features, shape))\n",
        "        \n",
        "\n",
        "    for j in range(nb_style_images):\n",
        "        sl = sl1[j] - sl2[j]\n",
        "\n",
        "        # Improvement 4\n",
        "        # Geometric weighted scaling of style loss\n",
        "        loss += (style_weights[j] / (2 ** (nb_layers - (i + 1)))) * sl\n",
        "\n",
        "loss += total_variation_weight * total_variation_loss(combination_image)\n",
        "\n",
        "# get the gradients of the generated image wrt the loss\n",
        "grads = K.gradients(loss, combination_image)\n",
        "\n",
        "outputs = [loss]\n",
        "if type(grads) in {list, tuple}:\n",
        "    outputs += grads\n",
        "else:\n",
        "    outputs.append(grads)\n",
        "\n",
        "f_outputs = K.function([combination_image], outputs)\n",
        "\n",
        "\n",
        "def eval_loss_and_grads(x):\n",
        "    x = x.reshape((1, img_width, img_height, 3))\n",
        "    outs = f_outputs([x])\n",
        "    loss_value = outs[0]\n",
        "    if len(outs[1:]) == 1:\n",
        "        grad_values = outs[1].flatten().astype('float64')\n",
        "    else:\n",
        "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
        "    return loss_value, grad_values\n",
        "\n",
        "\n",
        "# this Evaluator class makes it possible\n",
        "# to compute loss and gradients in one pass\n",
        "# while retrieving them via two separate functions,\n",
        "# \"loss\" and \"grads\". This is done because scipy.optimize\n",
        "# requires separate functions for loss and gradients,\n",
        "# but computing them separately would be inefficient.\n",
        "class Evaluator(object):\n",
        "    def __init__(self):\n",
        "        self.loss_value = None\n",
        "        self.grads_values = None\n",
        "\n",
        "    def loss(self, x):\n",
        "        assert self.loss_value is None\n",
        "        loss_value, grad_values = eval_loss_and_grads(x)\n",
        "        self.loss_value = loss_value\n",
        "        self.grad_values = grad_values\n",
        "        return self.loss_value\n",
        "\n",
        "    def grads(self, x):\n",
        "        assert self.loss_value is not None\n",
        "        grad_values = np.copy(self.grad_values)\n",
        "        self.loss_value = None\n",
        "        self.grad_values = None\n",
        "        return grad_values\n",
        "\n",
        "\n",
        "evaluator = Evaluator()\n",
        "\n",
        "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
        "# so as to minimize the neural style loss\n",
        "\n",
        "\n",
        "if \"content\" in init_image or \"gray\" in init_image:\n",
        "    x = preprocess_image(base_image_path, True, read_mode=read_mode)\n",
        "elif \"noise\" in init_image:\n",
        "    x = np.random.uniform(0, 255, (1, img_width, img_height, 3)) - 128.\n",
        "\n",
        "else:\n",
        "    print(\"Using initial image : \", init_image)\n",
        "    x = preprocess_image(init_image, read_mode=read_mode)\n",
        "\n",
        "\n",
        "prev_min_val = -1\n",
        "\n",
        "improvement_threshold = float(min_improvement)\n",
        "\n",
        "for i in range(num_iter):\n",
        "    print(\"Starting iteration %d of %d\" % ((i + 1), num_iter))\n",
        "    start_time = time.time()\n",
        "\n",
        "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20)\n",
        "\n",
        "    if prev_min_val == -1:\n",
        "        prev_min_val = min_val\n",
        "\n",
        "    improvement = (prev_min_val - min_val) / prev_min_val * 100\n",
        "\n",
        "    print(\"Current loss value:\", min_val, \" Improvement : %0.3f\" % improvement, \"%\")\n",
        "    prev_min_val = min_val\n",
        "    # save current generated image\n",
        "    img = deprocess_image(x.copy())\n",
        "\n",
        "    if not rescale_image:\n",
        "        img_ht = int(img_width * aspect_ratio)\n",
        "        print(\"Rescaling Image to (%d, %d)\" % (img_width, img_ht))\n",
        "        img = imresize(img, (img_width, img_ht), interp=rescale_method)\n",
        "\n",
        "    if rescale_image:\n",
        "        print(\"Rescaling Image to (%d, %d)\" % (img_WIDTH, img_HEIGHT))\n",
        "        img = imresize(img, (img_WIDTH, img_HEIGHT), interp=rescale_method)\n",
        "    \n",
        "    if i == num_iter-1:\n",
        "        fname = result_prefix + \".png\"\n",
        "\n",
        "        mask = load_mask_sil(inverted_silhouette, img.shape)\n",
        "        final_img = mask_content(base_image_path, img, mask)\n",
        "        end_time = time.time()\n",
        "        imsave(fname, final_img)\n",
        "        print(\"Image saved as\", fname)\n",
        "        print(\"Iteration %d completed in %ds\" % (i + 1, end_time - start_time))\n",
        "\n",
        "    if improvement_threshold is not 0.0:\n",
        "        if improvement < improvement_threshold and improvement is not 0.0:\n",
        "            print(\"Improvement (%f) is less than improvement threshold (%f). Early stopping script.\" %\n",
        "                  (improvement, improvement_threshold))\n",
        "            exit()"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 600, 600, 3)\n",
            "nb_tensors 3\n",
            "Model loaded.\n",
            "Starting iteration 1 of 25\n",
            "Current loss value: 226655060.0  Improvement : 0.000 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 2 of 25\n",
            "Current loss value: 91433760.0  Improvement : 59.660 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 3 of 25\n",
            "Current loss value: 51997240.0  Improvement : 43.131 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 4 of 25\n",
            "Current loss value: 35563640.0  Improvement : 31.605 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 5 of 25\n",
            "Current loss value: 28163432.0  Improvement : 20.808 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 6 of 25\n",
            "Current loss value: 22347186.0  Improvement : 20.652 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 7 of 25\n",
            "Current loss value: 19306760.0  Improvement : 13.605 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 8 of 25\n",
            "Current loss value: 16412426.0  Improvement : 14.991 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 9 of 25\n",
            "Current loss value: 14418033.0  Improvement : 12.152 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 10 of 25\n",
            "Current loss value: 12808242.0  Improvement : 11.165 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 11 of 25\n",
            "Current loss value: 11357252.0  Improvement : 11.329 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 12 of 25\n",
            "Current loss value: 10314228.0  Improvement : 9.184 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 13 of 25\n",
            "Current loss value: 9475031.0  Improvement : 8.136 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 14 of 25\n",
            "Current loss value: 8634168.0  Improvement : 8.875 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 15 of 25\n",
            "Current loss value: 7902504.0  Improvement : 8.474 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 16 of 25\n",
            "Current loss value: 7331326.0  Improvement : 7.228 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 17 of 25\n",
            "Current loss value: 6860604.0  Improvement : 6.421 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 18 of 25\n",
            "Current loss value: 6450300.0  Improvement : 5.981 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 19 of 25\n",
            "Current loss value: 6124539.5  Improvement : 5.050 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 20 of 25\n",
            "Current loss value: 5851914.5  Improvement : 4.451 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 21 of 25\n",
            "Current loss value: 5596662.0  Improvement : 4.362 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 22 of 25\n",
            "Current loss value: 5380618.0  Improvement : 3.860 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 23 of 25\n",
            "Current loss value: 5199050.0  Improvement : 3.374 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 24 of 25\n",
            "Current loss value: 5036204.5  Improvement : 3.132 %\n",
            "Rescaling Image to (600, 600)\n",
            "Starting iteration 25 of 25\n",
            "Current loss value: 4897289.0  Improvement : 2.758 %\n",
            "Rescaling Image to (600, 600)\n",
            "Image saved as /content/drive/My Drive/intricate-art-neural-transfer/EssenceStyles/styled-essence-137.png\n",
            "Iteration 25 completed in 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6-tbfkEm7eG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# home_dir = '/content/drive/My Drive/intricate-art-neural-transfer/'\n",
        "# num_iter = 25\n",
        "# for i in range(5, len(final_file_list)):\n",
        "#   image_name = final_file_list[i]\n",
        "#   index = int(final_file_list[i].split(\".\")[0].split(\"-\")[-1])\n",
        "#   base_image_path = home_dir + 'EssenceOutputs/' + image_name\n",
        "#   # base_image_path = home_dir + 'silhouettes/ballerina.jpg'\n",
        "#   style_reference_image_paths = [\"/content/drive/My Drive/intricate-art-neural-transfer/style/essence-\" + str(index) + \"-style.png\"]\n",
        "#   result_prefix = home_dir + 'EssenceStyles/styled-' + image_name.split(\".\")[0]\n",
        "#   intricate_style_transfer(base_image_path, style_reference_image_paths, result_prefix, num_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbV175ITOelD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image, display, HTML\n",
        "home_dir = '/content/drive/My Drive/intricate-art-neural-transfer/'\n",
        "\n",
        "for i in range(len(final_file_list)):\n",
        "    image_name = final_file_list[i]\n",
        "    index = int(final_file_list[i].split(\".\")[0].split(\"-\")[-1])\n",
        "    print(descriptions[index])\n",
        "    print(top3[index])\n",
        "    image1 = home_dir + 'EssenceOutputs/' + image_name\n",
        "    print(image1)\n",
        "    image2 = home_dir + \"style/essence-\" + str(index) + \"-style.png\"\n",
        "    image3 = home_dir + 'EssenceStyles/styled-' + image_name.split(\".\")[0]\n",
        "    display(HTML(\"<table><tr><td><img src='\"+ image1 + \"'></td><td><img src='\" + image2 + \"'></td><td><img src='\"+ image3 + \"'></td><td></td></tr></table>\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5Xabbfpuy5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(\"final_file_list.npy\", final_file_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieIy1g9mwmvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}